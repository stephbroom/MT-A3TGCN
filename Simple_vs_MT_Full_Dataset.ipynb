{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Simple Classifier vs multi-task\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5rM8Pw-RVzg"
      },
      "source": [
        "Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huKQlSq4FeKz",
        "outputId": "e2cd6ab6-b11e-4ea0-85df-476bc7be7c14"
      },
      "outputs": [],
      "source": [
        "#clean up\n",
        "%pip uninstall -y pyg pyg-lib torch-geometric torch-scatter torch-sparse \\\n",
        "  torch-cluster torch-spline-conv torch-geometric-temporal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YhOllK-aFhBP",
        "outputId": "f620b4be-08f7-44b9-8a60-cf63c12306a8"
      },
      "outputs": [],
      "source": [
        "#Pin Torch/CUDA to a wheel-rich combo (GPU: CUDA 12.1)\n",
        "%pip install --upgrade --force-reinstall \\\n",
        "  torch==2.4.0+cu121 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJfJXH68FiiX",
        "outputId": "81a2900e-d095-4344-ed9d-61c0a4c78265"
      },
      "outputs": [],
      "source": [
        "#compiled PyG addons that MATCH torch 2.4.0+cu121 (no builds)\n",
        "%pip install --no-cache-dir --only-binary=:all: \\\n",
        "  pyg-lib torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
        "  -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weURCJYgFmjA",
        "outputId": "65e42800-46d1-4624-945e-10a2e2a404d3"
      },
      "outputs": [],
      "source": [
        "#pure-Python core (fast, no compiling)\n",
        "%pip install --no-cache-dir torch_geometric==2.6.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo-ngSqyFoxM",
        "outputId": "414ebc09-914d-4405-91a4-2b8b15684961"
      },
      "outputs": [],
      "source": [
        "#temporal install (don't let pip re-resolve addons)\n",
        "%pip install --no-cache-dir --no-deps torch-geometric-temporal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXF86BzYFqZX",
        "outputId": "3d77692c-01d5-443d-e741-72c7271f21fd"
      },
      "outputs": [],
      "source": [
        "#check it worked\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.is_available())\n",
        "\n",
        "import torch_geometric, torch_scatter, torch_sparse, torch_cluster, torch_spline_conv\n",
        "print(\"PyG:\", torch_geometric.__version__,\n",
        "      \"| scatter:\", torch_scatter.__version__,\n",
        "      \"| sparse:\", torch_sparse.__version__)\n",
        "\n",
        "from torch_geometric_temporal.nn.recurrent import A3TGCN\n",
        "print(\"A3TGCN OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSpnX1DZeFG-",
        "outputId": "4fc1b014-810b-4d23-b93f-208bc94fcb3e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive # mount to drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWc2t6JZ75Z-"
      },
      "outputs": [],
      "source": [
        "#changing dir\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Final Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN47RXVUVoUb",
        "outputId": "b9da7390-ee19-4e72-fa2d-562ca81cf1ba"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# Temporal GNN\n",
        "from torch_geometric_temporal.nn.recurrent import A3TGCN\n",
        "\n",
        "# helpers\n",
        "from helper_functions import get_edge_data, compute_classification_metrics, compute_regression_metrics\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 123\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLwnvPMkVt-5"
      },
      "source": [
        "## Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR6sktBkVv-p"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "HIDDEN_DIM        = 256\n",
        "PERIODS           = 1\n",
        "LR                = 1e-2\n",
        "WEIGHT_DECAY      = 1e-5\n",
        "EPOCHS            = 200\n",
        "LOSS_ALPHA        = 0.5     # weight for classification loss (multi‑task only)\n",
        "BATCH_SIZE        = 1\n",
        "VAL_WINDOWS_COUNT = 4\n",
        "\n",
        "#optimiser split LRs (trunk / clf / reg)\n",
        "LR_TRUNK = LR * 0.\n",
        "LR_CLF   = LR\n",
        "LR_REG   = LR * 5.0\n",
        "\n",
        "PRETRAIN_EPOCHS = 50  #warm‑up for regression (set 0 for classifier‑only)\n",
        "PATIENCE        = 20   #early stop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSzfMpeYRiFE"
      },
      "source": [
        "## Loading data and defining datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FR70rsHtNSxC",
        "outputId": "1cc5b2f5-9447-4fe5-9e52-054226b045c9"
      },
      "outputs": [],
      "source": [
        "#load edges and timestamps and mappings\n",
        "edges_df = pd.read_csv(\"data/txs_edgelist.csv\", usecols=[\"txId1\",\"txId2\"])\n",
        "ts_df    = pd.read_csv(\"data/txs_features.csv\", usecols=[\"txId\",\"Time step\"]).rename(\n",
        "    columns={\"txId\":\"txId2\",\"Time step\":\"timestamp\"}\n",
        ")\n",
        "edges_ts = edges_df.merge(ts_df, on=\"txId2\", how=\"left\")\n",
        "\n",
        "verts    = pd.read_parquet(\"data/verts_int.parquet\").astype({\"txId\":str})\n",
        "int_map  = verts.set_index('txId')['int_id'].to_dict()\n",
        "\n",
        "edges_ts['src'] = edges_ts['txId1'].astype(str).map(int_map)\n",
        "edges_ts['dst'] = edges_ts['txId2'].astype(str).map(int_map)\n",
        "edges_ts = edges_ts[['src','dst','timestamp']]\n",
        "edges_ts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyVTANC_0_J8"
      },
      "outputs": [],
      "source": [
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, features_csv: str, classes_csv: str, verts_parquet: str, include_orbits: bool = True):\n",
        "        super().__init__()\n",
        "        self.include_orbits = include_orbits\n",
        "\n",
        "        df = pd.read_csv(features_csv)\n",
        "        # Orbit targets are in columns prefixed with 'orbit_'\n",
        "        self.orbit_cols = [c for c in df.columns if c.startswith('orbit_')]\n",
        "\n",
        "        #Meta/PCA feature columns (exclude ids, window, and orbit columns from this)\n",
        "        drop = ['int_id', 'window_start']\n",
        "        drop += self.orbit_cols  # X excludes orbit targets in both modes\n",
        "        self.meta_pca_cols = [c for c in df.columns if c not in drop]\n",
        "\n",
        "        self.df = df.copy()\n",
        "\n",
        "        #Attach class labels via txId to int_id mapping\n",
        "        classes = pd.read_csv(classes_csv, usecols=['txId','class']).astype({'txId':str})\n",
        "        verts   = pd.read_parquet(verts_parquet).astype({'txId':str})\n",
        "        cls     = verts.merge(classes, on='txId', how='left')[['int_id','class']]\n",
        "        cls['class'] = cls['class'].fillna(3).astype(int)   # unknown is 3\n",
        "        cls.loc[cls['class'] == 3, 'class'] = -100          # mask unknown\n",
        "        self.cls_map = dict(zip(cls['int_id'], cls['class']))\n",
        "\n",
        "        #sorted unique window starts\n",
        "        self.windows = sorted(self.df['window_start'].unique())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        w = self.windows[idx]\n",
        "        sub = self.df[self.df['window_start'] == w].sort_values('int_id')\n",
        "\n",
        "        #classification labels (masked unknowns = -100)\n",
        "        y_clf = torch.tensor([self.cls_map[i] for i in sub['int_id']], dtype=torch.long)\n",
        "\n",
        "        #if include_orbits=False, drop unlabelled rows within this window\n",
        "        if not self.include_orbits:\n",
        "            keep = (y_clf != -100)\n",
        "            sub = sub.loc[keep.cpu().numpy()].copy()\n",
        "            y_clf = y_clf[keep]\n",
        "\n",
        "        #Features X (meta/pca only; orbit targets are not features)\n",
        "        x_meta = torch.tensor(sub[self.meta_pca_cols].values, dtype=torch.float32)\n",
        "\n",
        "        #regression targets and previous window orbits (only if include_orbits = True)\n",
        "        if self.include_orbits and len(self.orbit_cols) > 0:\n",
        "            y_reg = torch.tensor(sub[self.orbit_cols].values, dtype=torch.float32)\n",
        "            if idx > 0:\n",
        "                w_prev = self.windows[idx-1]\n",
        "                prev = self.df[self.df['window_start'] == w_prev][['int_id'] + self.orbit_cols]\n",
        "                prev = prev.sort_values('int_id')\n",
        "                prev_orbits = torch.tensor(prev[self.orbit_cols].values, dtype=torch.float32)\n",
        "            else:\n",
        "                prev_orbits = torch.zeros_like(y_reg)\n",
        "        else:\n",
        "            # empty placeholders that collate cleanly for classification only\n",
        "            y_reg = torch.zeros((x_meta.size(0), 0), dtype=torch.float32)\n",
        "            prev_orbits = torch.zeros((x_meta.size(0), 0), dtype=torch.float32)\n",
        "\n",
        "        #scalars for window ids\n",
        "        window_start_t       = torch.tensor(int(w), dtype=torch.long)\n",
        "        window_prev_start_t  = torch.tensor(int(self.windows[idx-1]) if idx>0 else int(w), dtype=torch.long)\n",
        "\n",
        "        int_ids_list = [int(i) if i is not None else -1 for i in sub['int_id'].tolist()]\n",
        "\n",
        "        return {\n",
        "            'x': x_meta,\n",
        "            'y_reg': y_reg,\n",
        "            'y_clf': y_clf,\n",
        "            'prev_orbits': prev_orbits,\n",
        "            'window_start': window_start_t,\n",
        "            'window_prev_start': window_prev_start_t,\n",
        "            'int_ids': int_ids_list,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGM3K4_01WEF"
      },
      "outputs": [],
      "source": [
        "def build_dataloaders(\n",
        "    features_csv: str = 'data/selected_features_windows_scaled.csv',\n",
        "    classes_csv: str  = 'data/txs_classes.csv',\n",
        "    verts_parquet: str= 'data/verts_int.parquet',\n",
        "    batch_size: int = 1,\n",
        "    val_windows_count: int = 4,\n",
        "    include_orbits: bool = True,\n",
        "    # Extras\n",
        "    generator=None, worker_init_fn=None, collate_fn=None,\n",
        "    num_workers: int = 0, pin_memory: bool = False, shuffle_train: bool = True,\n",
        "):\n",
        "    dataset = WindowDataset(features_csv, classes_csv, verts_parquet, include_orbits=include_orbits)\n",
        "    total   = len(dataset)\n",
        "    val_n   = min(val_windows_count, total)\n",
        "    train_n = total - val_n\n",
        "\n",
        "    train_ds = Subset(dataset, list(range(train_n)))\n",
        "    val_ds   = Subset(dataset, list(range(train_n, total)))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=shuffle_train,\n",
        "        worker_init_fn=worker_init_fn, generator=generator,\n",
        "        collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False,\n",
        "        worker_init_fn=worker_init_fn, generator=generator,\n",
        "        collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    print(f'Total windows: {total}, Train: {train_n}, Val: {val_n} (last {val_windows_count})')\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4_OBLaHWM-Z",
        "outputId": "9009a521-a27f-4fee-ef36-6354787b9288"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader = build_dataloaders(\n",
        "    features_csv      = 'data/selected_features_windows_scaled.csv',\n",
        "    classes_csv       = 'data/txs_classes.csv',\n",
        "    verts_parquet     = 'data/verts_int.parquet',\n",
        "    batch_size        = BATCH_SIZE,\n",
        "    val_windows_count = VAL_WINDOWS_COUNT,\n",
        "    include_orbits    = True,    # Multi‑task\n",
        "    worker_init_fn    = seed_worker,\n",
        "    generator         = g,\n",
        ")\n",
        "\n",
        "# Shapes\n",
        "sample     = next(iter(train_loader))\n",
        "IN_FEATS   = sample['x'].shape[-1]\n",
        "NUM_ORBITS = (sample['y_reg'].shape[-1] if sample['y_reg'].numel() > 0 else len([c for c in pd.read_csv('data/selected_features_windows_scaled.csv').columns if c.startswith('orbit_')]))\n",
        "NUM_CLASSES= 2\n",
        "print(f'Features={IN_FEATS}, Orbits={NUM_ORBITS}, Classes={NUM_CLASSES}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fsaN1rBWQPh"
      },
      "outputs": [],
      "source": [
        "class MultiTaskA3TGCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_orbits, num_classes, periods=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_orbits = num_orbits\n",
        "        self.a3tgcn = A3TGCN(in_channels=in_channels, out_channels=hidden_channels, periods=periods)\n",
        "\n",
        "        # Regressor predicts delta, then we add prev_orbits (residual style)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_channels + num_orbits, hidden_channels * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels * 2, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_channels),\n",
        "            nn.Linear(hidden_channels, num_orbits),\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "        # Zero‑init final reg layer so we start near \"copy prev_orbits\"\n",
        "        with torch.no_grad():\n",
        "            last = self.regressor[-1]\n",
        "            last.weight.zero_()\n",
        "            last.bias.zero_()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, prev_orbits=None):\n",
        "        # A3TGCN expects [N, F, T]; we use T=1 here\n",
        "        x_t = x.unsqueeze(-1) if x.dim() == 2 else x\n",
        "        h   = self.a3tgcn(x_t, edge_index, edge_weight)  # [N, hidden]\n",
        "\n",
        "        if prev_orbits is None or (isinstance(prev_orbits, torch.Tensor) and prev_orbits.numel() == 0):\n",
        "            prev_orbits = torch.zeros(h.size(0), self.num_orbits, device=h.device, dtype=h.dtype)\n",
        "\n",
        "        z      = torch.cat([h, prev_orbits], dim=1)\n",
        "        delta  = self.regressor(z)\n",
        "        y_reg  = prev_orbits + delta\n",
        "        y_clf  = self.classifier(h)\n",
        "        return y_reg, y_clf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnRULuDbWSJ6",
        "outputId": "3b101023-f5ec-4a1e-af34-918743a458bc"
      },
      "outputs": [],
      "source": [
        "#instantiating model\n",
        "model = MultiTaskA3TGCN(\n",
        "    in_channels     = IN_FEATS,\n",
        "    hidden_channels = HIDDEN_DIM,\n",
        "    num_orbits      = NUM_ORBITS,\n",
        "    num_classes     = NUM_CLASSES,\n",
        "    periods         = PERIODS,\n",
        "    dropout         = 0.1,\n",
        ").to(device)\n",
        "\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "GF7cXF2OWvQz",
        "outputId": "960a7913-3e25-45a6-ba5f-84d538cb5b18"
      },
      "outputs": [],
      "source": [
        "# Losses\n",
        "criterion_reg = nn.SmoothL1Loss(beta=0.5, reduction='mean')\n",
        "criterion_clf = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "#Optimiser with split parameter groups\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.a3tgcn.parameters(), 'lr': LR_TRUNK, 'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': model.classifier.parameters(), 'lr': LR_CLF,   'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': model.regressor.parameters(),  'lr': LR_REG,   'weight_decay': 0.0},\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_YoHwBJIZeO"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-atJ0MPwpHI"
      },
      "outputs": [],
      "source": [
        "def _ensure_dir(path: str | None):\n",
        "    if path is None:\n",
        "        return\n",
        "    d = os.path.dirname(path)\n",
        "    if d and not os.path.exists(d):\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def _make_checkpoint(model, optimizer, epoch: int, train_stats: dict, val_stats: dict, do_regression: bool):\n",
        "    ckpt = {\n",
        "        'epoch': epoch,\n",
        "        'model_class': type(model).__name__,\n",
        "        'backbone': 'A3TGCN',\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'train': train_stats,\n",
        "        'val': val_stats,\n",
        "        'hparams': {\n",
        "            'HIDDEN_DIM': HIDDEN_DIM, 'PERIODS': PERIODS, 'LR': LR,\n",
        "            'WEIGHT_DECAY': WEIGHT_DECAY, 'EPOCHS': EPOCHS, 'LOSS_ALPHA': LOSS_ALPHA,\n",
        "            'BATCH_SIZE': BATCH_SIZE, 'VAL_WINDOWS_COUNT': VAL_WINDOWS_COUNT,\n",
        "            'LR_TRUNK': LR_TRUNK, 'LR_CLF': LR_CLF, 'LR_REG': LR_REG,\n",
        "            'PRETRAIN_EPOCHS': PRETRAIN_EPOCHS, 'PATIENCE': PATIENCE,\n",
        "            'do_regression': do_regression,\n",
        "            'SEED': SEED,\n",
        "        },\n",
        "        'cpu_rng_state': torch.random.get_rng_state(),\n",
        "        'cuda_rng_state': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "        'device': str(device),\n",
        "    }\n",
        "    return ckpt\n",
        "\n",
        "def compute_r2_per_orbit(y_true_np: np.ndarray, y_pred_np: np.ndarray):\n",
        "    \"\"\"\n",
        "    y_true_np, y_pred_np: shape [N_total, num_orbits]\n",
        "    Returns: list of R^2 per orbit (float, NaN if undefined)\n",
        "    \"\"\"\n",
        "    if y_true_np.size == 0 or y_pred_np.size == 0:\n",
        "        return []\n",
        "    if y_true_np.ndim == 1:\n",
        "        y_true_np = y_true_np[:, None]\n",
        "        y_pred_np = y_pred_np[:, None]\n",
        "\n",
        "    ss_res = np.sum((y_true_np - y_pred_np) ** 2, axis=0)\n",
        "    mu     = np.mean(y_true_np, axis=0)\n",
        "    ss_tot = np.sum((y_true_np - mu) ** 2, axis=0)\n",
        "\n",
        "    r2 = np.empty_like(ss_res, dtype=float)\n",
        "    mask = ss_tot > 0\n",
        "    r2[mask]  = 1.0 - (ss_res[mask] / (ss_tot[mask] + 1e-12))\n",
        "    r2[~mask] = np.nan\n",
        "    return r2.tolist()\n",
        "\n",
        "def run_epoch(model, loader, training: bool, do_regression: bool, alpha_clf: float):\n",
        "    if training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total, reg_sum, clf_sum = 0.0, 0.0, 0.0\n",
        "    all_true, all_probs = [], []\n",
        "    reg_true_chunks, reg_pred_chunks = [], []   #for per-orbit R^2\n",
        "\n",
        "    with torch.set_grad_enabled(training):\n",
        "        for batch in loader:\n",
        "            x       = batch['x'].squeeze(0).to(device, non_blocking=True)\n",
        "            y_clf0  = batch['y_clf'].squeeze(0).to(device, non_blocking=True)\n",
        "            y_reg   = batch['y_reg'].squeeze(0).to(device, non_blocking=True)\n",
        "            prev_orb= batch['prev_orbits']\n",
        "            prev_orb= (prev_orb.squeeze(0).to(device, non_blocking=True) if isinstance(prev_orb, torch.Tensor) else None)\n",
        "\n",
        "            #Remap labels keeping mask\n",
        "            y_clf = torch.full_like(y_clf0, -100)\n",
        "            y_clf[y_clf0 == 1] = 0\n",
        "            y_clf[y_clf0 == 2] = 1\n",
        "\n",
        "            #Build edges for window\n",
        "            int_ids = batch['int_ids']\n",
        "            id_map  = {int(nid): i for i, nid in enumerate(int_ids)}\n",
        "            w0      = int(batch['window_start'].item())\n",
        "            ei, ew_raw = get_edge_data(w0, id_map, edges_ts)\n",
        "            if ei.numel() == 0 or ew_raw.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            # Normalise weights to be in [0,1]\n",
        "            ew = (ew_raw - ew_raw.min()) / (ew_raw.max() - ew_raw.min() + 1e-6)\n",
        "            ei = ei.to(device, non_blocking=True)\n",
        "            ew = ew.to(device, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            y_reg_pred, y_clf_logits = model(x, ei, ew, prev_orbits=prev_orb)\n",
        "\n",
        "            #compute losses\n",
        "            if do_regression:\n",
        "                loss_reg = criterion_reg(y_reg_pred, y_reg)\n",
        "            else:\n",
        "                loss_reg = torch.tensor(0.0, device=device)\n",
        "\n",
        "            # Only compute classification loss if it contributes; otherwise keep it zero\n",
        "            loss_clf = criterion_clf(y_clf_logits, y_clf) if alpha_clf > 0 else torch.tensor(0.0, device=device)\n",
        "            loss = loss_reg + alpha_clf * loss_clf\n",
        "\n",
        "            if training:\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            total   += loss.item()\n",
        "            reg_sum += loss_reg.item() if do_regression else 0.0\n",
        "            clf_sum += (alpha_clf * loss_clf).item()   # log weighted CE (0 during pretrain)\n",
        "\n",
        "            # Collect clf metrics for labelled nodes only\n",
        "            mask = (y_clf != -100)\n",
        "            if mask.sum() > 0:\n",
        "                probs  = F.softmax(y_clf_logits[mask], dim=1)[:, 1].detach().cpu().numpy()\n",
        "                labels = y_clf[mask].detach().cpu().numpy()\n",
        "                all_probs.extend(probs.tolist())\n",
        "                all_true.extend(labels.tolist())\n",
        "\n",
        "            # Accumulate for per-orbit R^2\n",
        "            if do_regression and y_reg.numel() > 0:\n",
        "                reg_true_chunks.append(y_reg.detach().cpu().numpy())\n",
        "                reg_pred_chunks.append(y_reg_pred.detach().cpu().numpy())\n",
        "\n",
        "    #Aggregate classification metrics\n",
        "    if len(all_true) > 0 and len(set(all_true)) > 1:\n",
        "        auroc = roc_auc_score(all_true, all_probs)\n",
        "        auprc = average_precision_score(all_true, all_probs)\n",
        "        acc   = accuracy_score(all_true, (np.array(all_probs) >= 0.5).astype(int))\n",
        "        f1    = f1_score(all_true, (np.array(all_probs) >= 0.5).astype(int))\n",
        "    else:\n",
        "        auroc = auprc = acc = f1 = float('nan')\n",
        "\n",
        "    #Per-orbit R^2\n",
        "    if do_regression and len(reg_true_chunks) > 0:\n",
        "        y_true_np = np.vstack(reg_true_chunks)\n",
        "        y_pred_np = np.vstack(reg_pred_chunks)\n",
        "        r2_by_orbit = compute_r2_per_orbit(y_true_np, y_pred_np)\n",
        "    else:\n",
        "        r2_by_orbit = []\n",
        "\n",
        "    return {\n",
        "        'loss': total / max(len(loader), 1),\n",
        "        'reg_loss': reg_sum / max(len(loader), 1),\n",
        "        'clf_loss': clf_sum / max(len(loader), 1),\n",
        "        'auroc': auroc, 'auprc': auprc, 'acc': acc, 'f1': f1,\n",
        "        'r2_by_orbit': r2_by_orbit,\n",
        "    }\n",
        "\n",
        "def train_model(model, train_loader, val_loader, do_regression: bool, pretrain_epochs: int,\n",
        "                alpha_clf: float, checkpoint_path: str | None = None,\n",
        "                start_epoch: int = 1, max_epochs: int | None = None,\n",
        "                init_eval: bool = True):\n",
        "    \"\"\"\n",
        "    Works from scratch and when resuming after a checkpoint:\n",
        "      - start_epoch: epoch index to start from (use your loaded ckpt epoch+1)\n",
        "      - max_epochs: number of epochs to run in THIS call (defaults to EPOCHS)\n",
        "      - init_eval: if True, seeds 'best' with an initial validation at start_epoch-1 state\n",
        "    \"\"\"\n",
        "    best = {'val_loss': float('inf'), 'state': None, 'history': [], 'path': None}\n",
        "    epochs_no_improve = 0\n",
        "    _ensure_dir(checkpoint_path)\n",
        "\n",
        "    total_epochs = (max_epochs if max_epochs is not None else EPOCHS)\n",
        "    end_epoch    = start_epoch + total_epochs - 1\n",
        "\n",
        "    # Seed 'best' with a validation pass of the current state (useful when resuming)\n",
        "    if init_eval:\n",
        "        # Use the same warm-up logic for the very first validation depending on start_epoch\n",
        "        init_val_alpha = 0.0 if (do_regression and start_epoch <= pretrain_epochs) else alpha_clf\n",
        "        va0 = run_epoch(model, val_loader, training=False, do_regression=do_regression, alpha_clf=init_val_alpha)\n",
        "        best['val_loss'] = va0['loss']\n",
        "        # Keep current weights as \"best\" snapshot\n",
        "        best['state'] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        best['history'].append({'epoch': start_epoch - 1, 'train': None, 'val': va0})\n",
        "        print(f\"Init @ epoch {start_epoch-1}: val_loss={va0['loss']:.4f}, AUROC={va0['auroc']:.3f}, \"\n",
        "              f\"AUPRC={va0['auprc']:.3f}, F1={va0['f1']:.3f}\"\n",
        "              + (f\", R2μ(val)={np.nanmean([r for r in va0['r2_by_orbit'] if r==r]):.3f}\" if do_regression and va0['r2_by_orbit'] else \"\"))\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch + 1):\n",
        "        # warm-up only if we're still within pretrain window\n",
        "        if do_regression and epoch <= pretrain_epochs:\n",
        "            alpha_now = 0.0\n",
        "            val_alpha = 0.0\n",
        "        else:\n",
        "            alpha_now = alpha_clf\n",
        "            val_alpha = alpha_clf\n",
        "\n",
        "        tr = run_epoch(model, train_loader, training=True,  do_regression=do_regression, alpha_clf=alpha_now)\n",
        "        va = run_epoch(model, val_loader,   training=False, do_regression=do_regression, alpha_clf=val_alpha)\n",
        "\n",
        "        best['history'].append({'epoch': epoch, 'train': tr, 'val': va})\n",
        "\n",
        "        # Compact R² means for log\n",
        "        def _mean_safe(r2s):\n",
        "            arr = np.array([r for r in r2s if r == r])  # drop NaNs\n",
        "            return float(np.mean(arr)) if arr.size else float('nan')\n",
        "\n",
        "        msg = (f\"Epoch {epoch:3d}/{end_epoch} | \"\n",
        "               f\"train: loss={tr['loss']:.4f} (reg={tr['reg_loss']:.4f}, clf={tr['clf_loss']:.4f}) \"\n",
        "               f\"| val: loss={va['loss']:.4f}, AUROC={va['auroc']:.3f}, AUPRC={va['auprc']:.3f}, F1={va['f1']:.3f}\")\n",
        "        if do_regression:\n",
        "            msg += (f\" | R2μ(train)={_mean_safe(tr['r2_by_orbit']):.3f}, R2μ(val)={_mean_safe(va['r2_by_orbit']):.3f}\")\n",
        "        print(msg)\n",
        "\n",
        "        # Optionally print full per-orbit R^2 for val\n",
        "        if do_regression and va['r2_by_orbit']:\n",
        "            r2_str = \", \".join(f\"{r:.3f}\" if np.isfinite(r) else \"nan\" for r in va['r2_by_orbit'])\n",
        "            print(f\"    Val R2 per-orbit: [{r2_str}]\")\n",
        "\n",
        "        # Early stopping on val loss (+ save best checkpoint if improved)\n",
        "        if va['loss'] + 1e-6 < best['val_loss']:\n",
        "            best['val_loss'] = va['loss']\n",
        "            best['state'] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            epochs_no_improve = 0\n",
        "\n",
        "            if checkpoint_path is not None:\n",
        "                ckpt = _make_checkpoint(model, optimizer, epoch, tr, va, do_regression=do_regression)\n",
        "                torch.save(ckpt, checkpoint_path)\n",
        "                best['path'] = checkpoint_path\n",
        "                print(f\"[Saved best checkpoint → {checkpoint_path}]\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"Early stop at epoch {epoch} (no improvement for {PATIENCE} epochs)\")\n",
        "                break\n",
        "\n",
        "    #restoring best weights if needed\n",
        "    if best['state'] is not None:\n",
        "        model.load_state_dict(best['state'], strict=True)\n",
        "    else:\n",
        "        print(\"Warning: no best state captured; model left as-is.\")\n",
        "\n",
        "    return best\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTT122gF7lN1"
      },
      "source": [
        "## Training multi-task model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nBafBTERoZhv",
        "outputId": "9fbe04c8-dbf1-43d1-ea38-4de21827fbf5"
      },
      "outputs": [],
      "source": [
        "print('--- Training Multi‑Task (regression + classification) ---')\n",
        "\n",
        "mt_best = train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    do_regression=True,\n",
        "    pretrain_epochs=PRETRAIN_EPOCHS,\n",
        "    alpha_clf=LOSS_ALPHA,\n",
        "    checkpoint_path=\"/content/drive/MyDrive/Final Project/models/best_multi.pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzZsGeEjo4ww",
        "outputId": "e1a11a69-3f1c-4768-e7ba-f4ed0b2a3b0a"
      },
      "outputs": [],
      "source": [
        "# --- Resume Multi-Task from checkpoint  ---\n",
        "\n",
        "CKPT_PATH = \"/content/drive/MyDrive/Final Project/models/best_multi.pt\"\n",
        "\n",
        "# Explicitly set weights_only=False s\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device, weights_only=False)\n",
        "\n",
        "# Rebuild model exactly as before\n",
        "model = MultiTaskA3TGCN(\n",
        "    in_channels     = IN_FEATS,\n",
        "    hidden_channels = HIDDEN_DIM,\n",
        "    num_orbits      = NUM_ORBITS,\n",
        "    num_classes     = NUM_CLASSES,\n",
        "    periods         = PERIODS,\n",
        "    dropout         = 0.1,\n",
        ").to(device)\n",
        "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
        "\n",
        "# Recreate optimiser with same param groups\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.a3tgcn.parameters(), 'lr': LR_TRUNK, 'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': model.classifier.parameters(), 'lr': LR_CLF,   'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': model.regressor.parameters(),  'lr': LR_REG,   'weight_decay': 0.0},\n",
        "])\n",
        "optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "# --- Safe RNG restore ---\n",
        "def _to_byte_cpu_tensor(x):\n",
        "    \"\"\"Coerce x into a CPU torch.uint8 tensor.\"\"\"\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.detach().to('cpu', dtype=torch.uint8)\n",
        "    import numpy as np\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        return torch.tensor(list(x), dtype=torch.uint8)\n",
        "    if isinstance(x, (list, tuple, np.ndarray)):\n",
        "        return torch.as_tensor(x, dtype=torch.uint8, device='cpu')\n",
        "    return torch.as_tensor(x, dtype=torch.uint8, device='cpu')\n",
        "\n",
        "def _restore_cpu_rng(state):\n",
        "    if state is None:\n",
        "        return\n",
        "    try:\n",
        "        torch.random.set_rng_state(_to_byte_cpu_tensor(state))\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not restore CPU RNG: {e}\")\n",
        "\n",
        "def _restore_cuda_rng(states):\n",
        "    if not torch.cuda.is_available() or states is None:\n",
        "        return\n",
        "    try:\n",
        "        fixed = []\n",
        "        for s in states:\n",
        "            fixed.append(_to_byte_cpu_tensor(s))\n",
        "        torch.cuda.set_rng_state_all(fixed)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not restore CUDA RNG: {e}\")\n",
        "\n",
        "# Restore RNG states\n",
        "_restore_cpu_rng(ckpt.get(\"cpu_rng_state\"))\n",
        "_restore_cuda_rng(ckpt.get(\"cuda_rng_state\"))\n",
        "\n",
        "# Compute resume epoch\n",
        "start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
        "print(f\"Resuming from epoch {start_epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg6x9krRWzOn",
        "outputId": "ebde2e10-9262-4eaa-9e25-582a1db55207"
      },
      "outputs": [],
      "source": [
        "print('--- Training Multi‑Task (regression + classification) ---')\n",
        "PRETRAIN_EPOCHS = 0         # important when resuming\n",
        "EXTRA_EPOCHS    = 100\n",
        "PATIENCE = 20\n",
        "mt_best = train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    do_regression=True,\n",
        "    pretrain_epochs=PRETRAIN_EPOCHS,\n",
        "    alpha_clf=LOSS_ALPHA,\n",
        "    checkpoint_path=\"/content/drive/MyDrive/Final Project/models/best_multi.pt\",\n",
        "    start_epoch=start_epoch,\n",
        "    max_epochs=EXTRA_EPOCHS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwUcEcMa7vKz"
      },
      "source": [
        "## Training classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BzwSj8mxLEp",
        "outputId": "b23b1a76-0219-4a5e-b9a4-3c164cec0a62"
      },
      "outputs": [],
      "source": [
        "# Build loaders without orbit targets\n",
        "clf_train_loader, clf_val_loader = build_dataloaders(\n",
        "    features_csv      = 'data/selected_features_windows_scaled.csv',\n",
        "    classes_csv       = 'data/txs_classes.csv',\n",
        "    verts_parquet     = 'data/verts_int.parquet',\n",
        "    batch_size        = BATCH_SIZE,\n",
        "    val_windows_count = VAL_WINDOWS_COUNT,\n",
        "    include_orbits    = False,          # CLASSIFIER ONLY\n",
        "    worker_init_fn    = seed_worker,\n",
        "    generator         = g,\n",
        ")\n",
        "\n",
        "# (Optional) infer IN_FEATS defensively from these loaders\n",
        "_sample = next(iter(clf_train_loader))\n",
        "IN_FEATS = _sample['x'].shape[-1]\n",
        "NUM_CLASSES = 2  # as before; keep consistent with your label remap\n",
        "PATIENCE        = 20\n",
        "\n",
        "# Fresh model (same backbone/head dims for fair comparison)\n",
        "clf_model = MultiTaskA3TGCN(\n",
        "    in_channels     = IN_FEATS,\n",
        "    hidden_channels = HIDDEN_DIM,\n",
        "    num_orbits      = NUM_ORBITS,   # kept for shape; prev_orbits will be zero-filled internally\n",
        "    num_classes     = NUM_CLASSES,\n",
        "    periods         = PERIODS,\n",
        "    dropout         = 0.1,\n",
        ").to(device)\n",
        "\n",
        "# Losses\n",
        "criterion_reg = nn.SmoothL1Loss(beta=0.5, reduction='mean')\n",
        "criterion_clf = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "# IMPORTANT: reset the global optimizer used by run_epoch/train_model\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': clf_model.a3tgcn.parameters(), 'lr': LR_TRUNK, 'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': clf_model.classifier.parameters(), 'lr': LR_CLF,   'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': clf_model.regressor.parameters(),  'lr': LR_REG,   'weight_decay': 0.0},\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P42W0S5477Kx",
        "outputId": "5f9d9c9e-3530-4adc-cfbd-cabec4177073"
      },
      "outputs": [],
      "source": [
        "# Train classifier-only (no regression term, no warm-up)\n",
        "print('--- Training Classifier-Only ---')\n",
        "clf_best = train_model(\n",
        "    clf_model, clf_train_loader, clf_val_loader,\n",
        "    do_regression   = False,      # <-- key difference\n",
        "    pretrain_epochs = 0,          # no pretrain when regression is off\n",
        "    alpha_clf       = 1.0,        # full weight on CE\n",
        "    checkpoint_path = \"/content/drive/MyDrive/Final Project/models/best_classifier.pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBG5aGj48AbX"
      },
      "source": [
        "## Evaluating models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHa8JtSE9z-A",
        "outputId": "4eafe744-62a9-47dd-8b40-90c1cdce1470"
      },
      "outputs": [],
      "source": [
        "## loading the best multi task\n",
        "print('--- Load best MULTI-TASK for evaluation ---')\n",
        "\n",
        "# Build val loader with orbits\n",
        "_, mt_val_loader = build_dataloaders(\n",
        "    features_csv      = 'data/selected_features_windows_scaled.csv',\n",
        "    classes_csv       = 'data/txs_classes.csv',\n",
        "    verts_parquet     = 'data/verts_int.parquet',\n",
        "    batch_size        = BATCH_SIZE,\n",
        "    val_windows_count = VAL_WINDOWS_COUNT,\n",
        "    include_orbits    = True,    # key difference\n",
        "    worker_init_fn    = seed_worker,\n",
        "    generator         = g,\n",
        ")\n",
        "\n",
        "# Recreate model\n",
        "_sample = next(iter(mt_val_loader))\n",
        "IN_FEATS   = _sample['x'].shape[-1]\n",
        "NUM_CLASSES= 2\n",
        "\n",
        "mt_model = MultiTaskA3TGCN(\n",
        "    in_channels     = IN_FEATS,\n",
        "    hidden_channels = HIDDEN_DIM,\n",
        "    num_orbits      = NUM_ORBITS,\n",
        "    num_classes     = NUM_CLASSES,\n",
        "    periods         = PERIODS,\n",
        "    dropout         = 0.1,\n",
        ").to(device)\n",
        "\n",
        "# Load weights\n",
        "ckpt_mt = torch.load(\"/content/drive/MyDrive/Final Project/models/best_multi.pt\",\n",
        "                     map_location=device, weights_only=False)\n",
        "mt_model.load_state_dict(ckpt_mt[\"model_state\"], strict=True)\n",
        "mt_model.eval()\n",
        "\n",
        "# Point your generic names to this model/loader for the eval cell\n",
        "model      = mt_model\n",
        "val_loader = mt_val_loader\n",
        "\n",
        "# Use these in your run_epoch call:\n",
        "EVAL_DO_REG = True\n",
        "EVAL_ALPHA  = LOSS_ALPHA   # your hyperparam (e.g., 0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R9Him0qG8CHJ",
        "outputId": "c9f3641c-a0a2-44c2-ba23-d3420cebe861"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
        "\n",
        "va = run_epoch(model, val_loader, training=False, do_regression=EVAL_DO_REG, alpha_clf=EVAL_ALPHA)\n",
        "\n",
        "print(f\"MT — Val: loss={va['loss']:.4f}, AUROC={va['auroc']:.3f}, AUPRC={va['auprc']:.3f}, F1={va['f1']:.3f}\")\n",
        "\n",
        "# Confusion matrix on labelled nodes\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        x      = batch['x'].squeeze(0).to(device)\n",
        "        y_clf0 = batch['y_clf'].squeeze(0).to(device)\n",
        "        prev   = batch['prev_orbits']\n",
        "        prev   = (prev.squeeze(0).to(device) if isinstance(prev, torch.Tensor) else None)\n",
        "\n",
        "        y_clf = torch.full_like(y_clf0, -100); y_clf[y_clf0==1]=0; y_clf[y_clf0==2]=1\n",
        "\n",
        "        id_map = {int(nid): i for i, nid in enumerate(batch['int_ids'])}\n",
        "        w0     = int(batch['window_start'].item())\n",
        "        ei, ew_raw = get_edge_data(w0, id_map, edges_ts)\n",
        "        if ei.numel()==0 or ew_raw.numel()==0:\n",
        "            continue\n",
        "        ew = (ew_raw - ew_raw.min()) / (ew_raw.max() - ew_raw.min() + 1e-6)\n",
        "\n",
        "        y_reg_pred, logits = model(x, ei.to(device), ew.to(device), prev_orbits=prev)\n",
        "        mask = (y_clf != -100)\n",
        "        if mask.sum() > 0:\n",
        "            all_true.extend(y_clf[mask].cpu().numpy().tolist())\n",
        "            all_pred.extend(F.softmax(logits[mask], dim=1)[:,1].cpu().numpy().tolist())\n",
        "\n",
        "if len(all_true)>0 and len(set(all_true))>1:\n",
        "    fpr, tpr, _ = roc_curve(all_true, all_pred)\n",
        "    plt.figure(); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.title('MT ROC'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.show()\n",
        "\n",
        "    prec, rec, _ = precision_recall_curve(all_true, all_pred)\n",
        "    plt.figure(); plt.plot(rec,prec); plt.title('MT PR'); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.show()\n",
        "\n",
        "cm = confusion_matrix(all_true, (np.array(all_pred)>=0.5).astype(int)) if len(all_true)>0 else np.array([[0,0],[0,0]])\n",
        "plt.figure(figsize=(5,4)); sns.heatmap(cm, annot=True, fmt='d'); plt.title('MT Confusion Matrix'); plt.xlabel('Pred'); plt.ylabel('True'); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPZ5jgCd92Z2",
        "outputId": "f0b2da0f-627a-40ea-b147-371c549d77da"
      },
      "outputs": [],
      "source": [
        "## loading best classification\n",
        "print('--- Load best CLASSIFIER for evaluation ---')\n",
        "\n",
        "# Build val loader without orbit targets\n",
        "_, clf_val_loader = build_dataloaders(\n",
        "    features_csv      = 'data/selected_features_windows_scaled.csv',\n",
        "    classes_csv       = 'data/txs_classes.csv',\n",
        "    verts_parquet     = 'data/verts_int.parquet',\n",
        "    batch_size        = BATCH_SIZE,\n",
        "    val_windows_count = VAL_WINDOWS_COUNT,\n",
        "    include_orbits    = False,   # key difference\n",
        "    worker_init_fn    = seed_worker,\n",
        "    generator         = g,\n",
        ")\n",
        "\n",
        "# Recreate model (same dims you trained with)\n",
        "_sample = next(iter(clf_val_loader))\n",
        "IN_FEATS   = _sample['x'].shape[-1]\n",
        "NUM_CLASSES= 2\n",
        "\n",
        "clf_model = MultiTaskA3TGCN(\n",
        "    in_channels     = IN_FEATS,\n",
        "    hidden_channels = HIDDEN_DIM,\n",
        "    num_orbits      = NUM_ORBITS,   # ok: prev_orbits will be zero-filled\n",
        "    num_classes     = NUM_CLASSES,\n",
        "    periods         = PERIODS,\n",
        "    dropout         = 0.1,\n",
        ").to(device)\n",
        "\n",
        "# Load weights\n",
        "ckpt_clf = torch.load(\"/content/drive/MyDrive/Final Project/models/best_classifier.pt\",\n",
        "                      map_location=device, weights_only=False)\n",
        "clf_model.load_state_dict(ckpt_clf[\"model_state\"], strict=True)\n",
        "clf_model.eval()\n",
        "\n",
        "# Point your generic names to this model/loader for the eval cell\n",
        "model      = clf_model\n",
        "val_loader = clf_val_loader\n",
        "\n",
        "# Use these in your run_epoch call:\n",
        "EVAL_DO_REG = False\n",
        "EVAL_ALPHA  = 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vDX7HdnwDL0n",
        "outputId": "02a767e6-de20-4706-c622-7ad16b25e90e"
      },
      "outputs": [],
      "source": [
        "va = run_epoch(model, val_loader, training=False, do_regression=EVAL_DO_REG, alpha_clf=EVAL_ALPHA)\n",
        "\n",
        "print(f\"MT — Val: loss={va['loss']:.4f}, AUROC={va['auroc']:.3f}, AUPRC={va['auprc']:.3f}, F1={va['f1']:.3f}\")\n",
        "\n",
        "# Confusion matrix on labelled nodes\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        x      = batch['x'].squeeze(0).to(device)\n",
        "        y_clf0 = batch['y_clf'].squeeze(0).to(device)\n",
        "        prev   = batch['prev_orbits']\n",
        "        prev   = (prev.squeeze(0).to(device) if isinstance(prev, torch.Tensor) else None)\n",
        "\n",
        "        y_clf = torch.full_like(y_clf0, -100); y_clf[y_clf0==1]=0; y_clf[y_clf0==2]=1\n",
        "\n",
        "        id_map = {int(nid): i for i, nid in enumerate(batch['int_ids'])}\n",
        "        w0     = int(batch['window_start'].item())\n",
        "        ei, ew_raw = get_edge_data(w0, id_map, edges_ts)\n",
        "        if ei.numel()==0 or ew_raw.numel()==0:\n",
        "            continue\n",
        "        ew = (ew_raw - ew_raw.min()) / (ew_raw.max() - ew_raw.min() + 1e-6)\n",
        "\n",
        "        y_reg_pred, logits = model(x, ei.to(device), ew.to(device), prev_orbits=prev)\n",
        "        mask = (y_clf != -100)\n",
        "        if mask.sum() > 0:\n",
        "            all_true.extend(y_clf[mask].cpu().numpy().tolist())\n",
        "            all_pred.extend(F.softmax(logits[mask], dim=1)[:,1].cpu().numpy().tolist())\n",
        "\n",
        "if len(all_true)>0 and len(set(all_true))>1:\n",
        "    fpr, tpr, _ = roc_curve(all_true, all_pred)\n",
        "    plt.figure(); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.title('CL ROC'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.show()\n",
        "\n",
        "    prec, rec, _ = precision_recall_curve(all_true, all_pred)\n",
        "    plt.figure(); plt.plot(rec,prec); plt.title('CL PR'); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.show()\n",
        "\n",
        "cm = confusion_matrix(all_true, (np.array(all_pred)>=0.5).astype(int)) if len(all_true)>0 else np.array([[0,0],[0,0]])\n",
        "plt.figure(figsize=(5,4)); sns.heatmap(cm, annot=True, fmt='d'); plt.title('CL Confusion Matrix'); plt.xlabel('Pred'); plt.ylabel('True'); plt.tight_layout(); plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
